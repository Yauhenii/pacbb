window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "core", "modulename": "core", "kind": "module", "doc": "<h2 id=\"overview\">Overview</h2>\n\n<p>The <code>core</code> package provides foundational components for the PAC-Bayes framework.</p>\n\n<h2 id=\"subpackages-and-modules\">Subpackages and Modules</h2>\n\n<ul>\n<li><strong>distribution/</strong> for modeling probability distributions over NN parameters</li>\n<li><strong>layer/</strong> for probabilistic neural network layers</li>\n<li><strong>bound/</strong> for PAC-Bayes bounds</li>\n<li><strong>objective/</strong> for combining empirical loss and KL terms</li>\n<li><strong>split_strategy/</strong> for partitioning datasets (prior/posterior/bound)</li>\n<li><strong>utils/</strong> for miscellaneous helpers</li>\n<li>Higher-level modules (<em>loss.py</em>, <em>metric.py</em>, <em>model.py</em>, <em>risk.py</em>, <em>training.py</em>)\nthat stitch these components together.</li>\n</ul>\n\n<h2 id=\"usage\">Usage</h2>\n\n<p>By assembling the pieces within <code>core</code>, you can build, train, and evaluate\nprobabilistic neural networks under the PAC-Bayes paradigm.</p>\n\n<h2 id=\"links\">Links</h2>\n\n<ul>\n<li><strong>Documentation</strong>: <a href=\"https://yauhenii.github.io/pacbb/core.html\">https://yauhenii.github.io/pacbb/core.html</a></li>\n<li><strong>PyPI</strong>: <a href=\"https://pypi.org/project/pacbb/\">https://pypi.org/project/pacbb/</a></li>\n<li><strong>Source Code</strong>: <a href=\"https://github.com/yauhenii/pacbb\">https://github.com/yauhenii/pacbb</a></li>\n</ul>\n"}, {"fullname": "core.bound", "modulename": "core.bound", "kind": "module", "doc": "<h2 id=\"overview\">Overview</h2>\n\n<p>This subpackage contains classes and functions for computing PAC-Bayes bounds.</p>\n\n<h2 id=\"contents\">Contents</h2>\n\n<ul>\n<li><strong>Concrete bounds</strong> like KLBound, McAllisterBound</li>\n<li><strong>Interfaces</strong> or base classes for creating custom bounds</li>\n</ul>\n\n<p>Use these bounds to estimate or certify generalization risk after training a \nprobabilistic neural network.</p>\n"}, {"fullname": "core.bound.AbstractBound", "modulename": "core.bound.AbstractBound", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.bound.AbstractBound.AbstractBound", "modulename": "core.bound.AbstractBound", "qualname": "AbstractBound", "kind": "class", "doc": "<p>Abstract PAC bound class for evaluating risk certificates.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>bound_delta (float):</strong>  Confidence level over random data samples.\nIt represents the probability that the upper bound of the PAC bound holds.</li>\n<li><strong>loss_delta (float):</strong>  Confidence level over random weight samples.\nIt represents the probability that the upper bound of empirical loss holds.</li>\n</ul>\n\n<p>Overall probability is (1 - loss_bound) - bound_delta.</p>\n\n<h6 id=\"attributes\">Attributes:</h6>\n\n<ul>\n<li><strong>_bound_delta (float):</strong>  Confidence level over random data samples.</li>\n<li><strong>_loss_delta (float):</strong>  Confidence level over random weight samples.</li>\n</ul>\n", "bases": "abc.ABC"}, {"fullname": "core.bound.AbstractBound.AbstractBound.calculate", "modulename": "core.bound.AbstractBound", "qualname": "AbstractBound.calculate", "kind": "function", "doc": "<p>Calculates the PAC Bayes bound.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>args:</strong>  Variable length argument list.</li>\n<li><strong>kwargs:</strong>  Arbitrary keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tuple[Union[Tensor, float], Union[Tensor, float]]:\n      A tuple containing the calculated PAC bound and the upper bound of empirical risk.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"o\">*</span><span class=\"n\">args</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">],</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.bound.KLBound", "modulename": "core.bound.KLBound", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.bound.KLBound.KLBound", "modulename": "core.bound.KLBound", "qualname": "KLBound", "kind": "class", "doc": "<p>Implements a PAC Bayes KL bound.</p>\n", "bases": "core.bound.AbstractBound.AbstractBound"}, {"fullname": "core.bound.KLBound.KLBound.__init__", "modulename": "core.bound.KLBound", "qualname": "KLBound.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">bound_delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">loss_delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "core.bound.KLBound.KLBound.calculate", "modulename": "core.bound.KLBound", "qualname": "KLBound.calculate", "kind": "function", "doc": "<p>Calculates the PAC Bayes bound.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>avg_loss (float):</strong>  The loss averaged using Monte Carlo sampling.</li>\n<li><strong>kl (Union[Tensor, float]):</strong>  The Kullback-Leibler divergence between prior and posterior distributions.</li>\n<li><strong>num_samples_bound (int):</strong>  The number of data samples in the bound dataset.</li>\n<li><strong>num_samples_loss (int):</strong>  The number of Monte Carlo samples.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tuple[Union[Tensor, float], Union[Tensor, float]]:\n      A tuple containing the calculated PAC Bayes bound and the upper bound of empirical risk.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">avg_loss</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">kl</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples_bound</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples_loss</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">],</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.bound.McAllesterBound", "modulename": "core.bound.McAllesterBound", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.bound.McAllesterBound.McAllesterBound", "modulename": "core.bound.McAllesterBound", "qualname": "McAllesterBound", "kind": "class", "doc": "<p>Implements a McAllester PAC Bayes bound.</p>\n", "bases": "core.bound.AbstractBound.AbstractBound"}, {"fullname": "core.bound.McAllesterBound.McAllesterBound.__init__", "modulename": "core.bound.McAllesterBound", "qualname": "McAllesterBound.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">bound_delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">loss_delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "core.bound.McAllesterBound.McAllesterBound.calculate", "modulename": "core.bound.McAllesterBound", "qualname": "McAllesterBound.calculate", "kind": "function", "doc": "<p>Calculates the PAC Bayes bound.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>avg_loss (float):</strong>  The loss averaged using Monte Carlo sampling.</li>\n<li><strong>kl (Union[Tensor, float]):</strong>  The Kullback-Leibler divergence between prior and posterior distributions.</li>\n<li><strong>num_samples_bound (int):</strong>  The number of data samples in the bound dataset.</li>\n<li><strong>num_samples_loss (int):</strong>  The number of Monte Carlo samples.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tuple[Union[Tensor, float], Union[Tensor, float]]:\n      A tuple containing the calculated PAC Bayes bound and the upper bound of empirical risk.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">avg_loss</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">kl</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples_bound</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples_loss</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">],</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution", "modulename": "core.distribution", "kind": "module", "doc": "<h2 id=\"overview\">Overview</h2>\n\n<p>Provides classes and utilities for representing probability distributions \nover neural network parameters.</p>\n\n<h2 id=\"key-components\">Key Components</h2>\n\n<ul>\n<li><strong>AbstractVariable</strong>: a base interface for distribution variables \n(e.g., GaussianVariable).</li>\n<li><strong>Utility functions</strong> for constructing, copying, and computing KL divergences \nbetween distributions.</li>\n</ul>\n\n<p>These distributions form the core representation of uncertainty in a \nPAC-Bayes model.</p>\n"}, {"fullname": "core.distribution.AbstractVariable", "modulename": "core.distribution.AbstractVariable", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.distribution.AbstractVariable.AbstractVariable", "modulename": "core.distribution.AbstractVariable", "qualname": "AbstractVariable", "kind": "class", "doc": "<p>An abstract class representing a single random variable for a probabilistic\nneural network parameter (e.g., weight or bias).</p>\n\n<h6 id=\"each-variable-holds\">Each variable holds:</h6>\n\n<blockquote>\n  <ul>\n  <li>A mean parameter (<code>mu</code>)</li>\n  <li>A <code>rho</code> parameter that is used to derive the standard deviation (<code>sigma</code>)</li>\n  <li>A method to sample from the underlying distribution</li>\n  <li>A method to compute KL divergence with another variable of the same type</li>\n  </ul>\n</blockquote>\n\n<p>This class inherits from <code>nn.Module</code> for parameter registration in PyTorch \nand from <code>KLDivergenceInterface</code> for consistent KL divergence handling.</p>\n", "bases": "torch.nn.modules.module.Module, core.utils.kl.KLDivergenceInterface, abc.ABC"}, {"fullname": "core.distribution.AbstractVariable.AbstractVariable.__init__", "modulename": "core.distribution.AbstractVariable", "qualname": "AbstractVariable.__init__", "kind": "function", "doc": "<p>Initialize an AbstractVariable with given <code>mu</code> and <code>rho</code> tensors.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mu (torch.Tensor):</strong>  The mean parameter for the distribution.</li>\n<li><strong>rho (torch.Tensor):</strong>  The parameter from which we derive sigma = log(1 + exp(rho)).</li>\n<li><strong>mu_requires_grad (bool, optional):</strong>  If True, allow gradients on <code>mu</code>.</li>\n<li><strong>rho_requires_grad (bool, optional):</strong>  If True, allow gradients on <code>rho</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mu</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">rho</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">mu_requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rho_requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "core.distribution.AbstractVariable.AbstractVariable.mu", "modulename": "core.distribution.AbstractVariable", "qualname": "AbstractVariable.mu", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "core.distribution.AbstractVariable.AbstractVariable.rho", "modulename": "core.distribution.AbstractVariable", "qualname": "AbstractVariable.rho", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "core.distribution.AbstractVariable.AbstractVariable.kl_div", "modulename": "core.distribution.AbstractVariable", "qualname": "AbstractVariable.kl_div", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "core.distribution.AbstractVariable.AbstractVariable.sigma", "modulename": "core.distribution.AbstractVariable", "qualname": "AbstractVariable.sigma", "kind": "variable", "doc": "<p>The standard deviation of the distribution, computed as:\n    sigma = log(1 + exp(rho)).</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor: A tensor representing the current standard deviation.</p>\n</blockquote>\n", "annotation": ": torch.Tensor"}, {"fullname": "core.distribution.AbstractVariable.AbstractVariable.sample", "modulename": "core.distribution.AbstractVariable", "qualname": "AbstractVariable.sample", "kind": "function", "doc": "<p>Draw a sample from this variable's underlying distribution.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor: A sampled value of the same shape as <code>mu</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.AbstractVariable.AbstractVariable.compute_kl", "modulename": "core.distribution.AbstractVariable", "qualname": "AbstractVariable.compute_kl", "kind": "function", "doc": "<p>Compute the KL divergence between this variable and another variable\nof the same distribution type.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>other (AbstractVariable):</strong>  Another AbstractVariable instance\nwith comparable parameters (e.g., mu, rho).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor: A scalar tensor with the KL divergence value.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">other</span><span class=\"p\">:</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.GaussianVariable", "modulename": "core.distribution.GaussianVariable", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.distribution.GaussianVariable.GaussianVariable", "modulename": "core.distribution.GaussianVariable", "qualname": "GaussianVariable", "kind": "class", "doc": "<p>Represents a Gaussian random variable with mean mu and rho.</p>\n", "bases": "core.distribution.AbstractVariable.AbstractVariable"}, {"fullname": "core.distribution.GaussianVariable.GaussianVariable.__init__", "modulename": "core.distribution.GaussianVariable", "qualname": "GaussianVariable.__init__", "kind": "function", "doc": "<p>Initialize the GaussianVariable.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mu (Tensor):</strong>  The mean of the Gaussian distribution.</li>\n<li><strong>rho (Tensor):</strong>  rho = log(exp(sigma)-1) where sigma is a standard deviation of the Gaussian distribution.</li>\n<li><strong>mu_requires_grad (bool):</strong>  Flag indicating whether mu is fixed.</li>\n<li><strong>rho_requires_grad (bool):</strong>  Flag indicating whether rho is fixed.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mu</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">rho</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">mu_requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rho_requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "core.distribution.GaussianVariable.GaussianVariable.sample", "modulename": "core.distribution.GaussianVariable", "qualname": "GaussianVariable.sample", "kind": "function", "doc": "<p>Sample from the Gaussian distribution.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: Sampled values from the Gaussian distribution.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.GaussianVariable.GaussianVariable.compute_kl", "modulename": "core.distribution.GaussianVariable", "qualname": "GaussianVariable.compute_kl", "kind": "function", "doc": "<p>Compute the KL divergence between two Gaussian distributions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>other (GaussianVariable):</strong>  The other Gaussian distribution.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: The KL divergence between the two distributions.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">other</span><span class=\"p\">:</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">GaussianVariable</span><span class=\"o\">.</span><span class=\"n\">GaussianVariable</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.LaplaceVariable", "modulename": "core.distribution.LaplaceVariable", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.distribution.LaplaceVariable.LaplaceVariable", "modulename": "core.distribution.LaplaceVariable", "qualname": "LaplaceVariable", "kind": "class", "doc": "<p>An abstract class representing a single random variable for a probabilistic\nneural network parameter (e.g., weight or bias).</p>\n\n<h6 id=\"each-variable-holds\">Each variable holds:</h6>\n\n<blockquote>\n  <ul>\n  <li>A mean parameter (<code>mu</code>)</li>\n  <li>A <code>rho</code> parameter that is used to derive the standard deviation (<code>sigma</code>)</li>\n  <li>A method to sample from the underlying distribution</li>\n  <li>A method to compute KL divergence with another variable of the same type</li>\n  </ul>\n</blockquote>\n\n<p>This class inherits from <code>nn.Module</code> for parameter registration in PyTorch \nand from <code>KLDivergenceInterface</code> for consistent KL divergence handling.</p>\n", "bases": "core.distribution.AbstractVariable.AbstractVariable"}, {"fullname": "core.distribution.LaplaceVariable.LaplaceVariable.__init__", "modulename": "core.distribution.LaplaceVariable", "qualname": "LaplaceVariable.__init__", "kind": "function", "doc": "<p>Initialize the LaplaceVariable.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mu (Tensor):</strong>  The mean of the Laplace distribution.</li>\n<li><strong>rho (Tensor):</strong>  rho = log(exp(sigma)-1) of the Laplace distribution.</li>\n<li><strong>mu_requires_grad (bool):</strong>  Flag indicating whether mu is fixed.</li>\n<li><strong>rho_requires_grad (bool):</strong>  Flag indicating whether rho is fixed.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mu</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">rho</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">mu_requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">rho_requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "core.distribution.LaplaceVariable.LaplaceVariable.sample", "modulename": "core.distribution.LaplaceVariable", "qualname": "LaplaceVariable.sample", "kind": "function", "doc": "<p>Sample from the Laplace distribution.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: Sampled values from the Laplace distribution.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.LaplaceVariable.LaplaceVariable.compute_kl", "modulename": "core.distribution.LaplaceVariable", "qualname": "LaplaceVariable.compute_kl", "kind": "function", "doc": "<p>Compute the KL divergence between two Laplace distributions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>other (LaplaceVariable):</strong>  The other Laplace distribution.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: The KL divergence between the two distributions.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">other</span><span class=\"p\">:</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">LaplaceVariable</span><span class=\"o\">.</span><span class=\"n\">LaplaceVariable</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.utils", "modulename": "core.distribution.utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.distribution.utils.DistributionT", "modulename": "core.distribution.utils", "qualname": "DistributionT", "kind": "variable", "doc": "<p></p>\n", "default_value": "typing.Dict[typing.Tuple[str, ...], typing.Dict[str, core.distribution.AbstractVariable.AbstractVariable]]"}, {"fullname": "core.distribution.utils.from_ivon", "modulename": "core.distribution.utils", "qualname": "from_ivon", "kind": "function", "doc": "<p>Construct a distribution from an IVON optimizer's parameters and Hessian approximations.</p>\n\n<p>This function extracts weight and bias information (as well as Hessians) from the\nIVON optimizer, then creates instances of the specified <code>distribution</code> for each\nparameter. The newly created distributions can be used as a posterior or prior\nin a PAC-Bayes setting.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The (deterministic) model whose parameters correspond to the IVON's weights.</li>\n<li><strong>optimizer (ivon.IVON):</strong>  An instance of IVON optimizer containing the Hessian approximations.</li>\n<li><strong>distribution (Type[AbstractVariable]):</strong>  The subclass of <code>AbstractVariable</code> to instantiate.</li>\n<li><strong>requires_grad (bool, optional):</strong>  If True, gradients will be computed on the newly created parameters.</li>\n<li><strong>get_layers_func (Callable, optional):</strong>  A function to retrieve model layers. Defaults to <code>get_torch_layers</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>DistributionT: A dictionary mapping layer names to dicts of {'weight': ..., 'bias': ...},\n  each containing an instance of <code>AbstractVariable</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"p\">:</span> <span class=\"n\">ivon</span><span class=\"o\">.</span><span class=\"n\">_ivon</span><span class=\"o\">.</span><span class=\"n\">IVON</span>,</span><span class=\"param\">\t<span class=\"n\">distribution</span><span class=\"p\">:</span> <span class=\"n\">Type</span><span class=\"p\">[</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">get_layers_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">],</span> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">get_torch_layers</span><span class=\"o\">&gt;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.utils.from_flat_rho", "modulename": "core.distribution.utils", "qualname": "from_flat_rho", "kind": "function", "doc": "<p>Create distributions for each layer using a shared or flat <code>rho</code> array.</p>\n\n<p>This function takes a model and a <code>rho</code> tensor/list containing values for all weight/bias\nelements in consecutive order. Each layer's <code>mu</code> is initialized from the current layer weights,\nand <code>rho</code> is reshaped accordingly for the layer's shape.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The PyTorch model whose layers are converted into distributions.</li>\n<li><strong>rho (Union[Tensor, List[float]]):</strong>  A 1D tensor or list of floating values used to set <code>rho</code>.</li>\n<li><strong>distribution (Type[AbstractVariable]):</strong>  The subclass of <code>AbstractVariable</code> to instantiate.</li>\n<li><strong>requires_grad (bool, optional):</strong>  If True, gradients will be computed for the distribution parameters.</li>\n<li><strong>get_layers_func (Callable, optional):</strong>  Function for iterating over the model layers. Defaults to <code>get_torch_layers</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>DistributionT: A dictionary of layer distributions keyed by layer names, each\n  containing 'weight' and 'bias' distributions if they exist.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">rho</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">distribution</span><span class=\"p\">:</span> <span class=\"n\">Type</span><span class=\"p\">[</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">get_layers_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">],</span> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">get_torch_layers</span><span class=\"o\">&gt;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.utils.from_random", "modulename": "core.distribution.utils", "qualname": "from_random", "kind": "function", "doc": "<p>Create a distribution for each layer with randomly initialized mean (using truncated normal)\nand a constant <code>rho</code> value.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The target PyTorch model.</li>\n<li><strong>rho (Tensor):</strong>  A single scalar tensor defining the initial <code>rho</code> for all weights/biases.</li>\n<li><strong>distribution (Type[AbstractVariable]):</strong>  The class for creating each weight/bias distribution.</li>\n<li><strong>requires_grad (bool, optional):</strong>  If True, allows gradient-based updates of <code>mu</code> and <code>rho</code>.</li>\n<li><strong>get_layers_func (Callable, optional):</strong>  Function to iterate over model layers. Defaults to <code>get_torch_layers</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>DistributionT: A dictionary containing layer-wise distributions for weights and biases.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">rho</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">distribution</span><span class=\"p\">:</span> <span class=\"n\">Type</span><span class=\"p\">[</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">get_layers_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">],</span> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">get_torch_layers</span><span class=\"o\">&gt;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.utils.from_zeros", "modulename": "core.distribution.utils", "qualname": "from_zeros", "kind": "function", "doc": "<p>Create distributions for each layer by setting <code>mu</code> to zero and <code>rho</code> to a constant value.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The PyTorch model.</li>\n<li><strong>rho (Tensor):</strong>  A scalar defining the initial <code>rho</code> for all weights/biases.</li>\n<li><strong>distribution (Type[AbstractVariable]):</strong>  Distribution class to instantiate.</li>\n<li><strong>requires_grad (bool, optional):</strong>  Whether to track gradients for <code>mu</code> and <code>rho</code>.</li>\n<li><strong>get_layers_func (Callable, optional):</strong>  Layer iteration function. Defaults to <code>get_torch_layers</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>DistributionT: A dictionary mapping layer names to weight/bias distributions.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">rho</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">distribution</span><span class=\"p\">:</span> <span class=\"n\">Type</span><span class=\"p\">[</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">get_layers_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">],</span> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">get_torch_layers</span><span class=\"o\">&gt;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.utils.from_layered", "modulename": "core.distribution.utils", "qualname": "from_layered", "kind": "function", "doc": "<p>Create distributions by extracting <code>mu</code> and <code>rho</code> from specified attributes in the model layers.</p>\n\n<p>This function looks up layer attributes for weight and bias (e.g., \"weight_mu\", \"weight_rho\")\nusing <code>attribute_mapping</code>, then initializes each distribution accordingly.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The model whose layers contain the specified attributes.</li>\n<li><strong>attribute_mapping (dict[str, str]):</strong>  A mapping of attribute names, for example:\n{\n  \"weight_mu\": \"mu_weight\",\n  \"weight_rho\": \"rho_weight\",\n  \"bias_mu\": \"mu_bias\",\n  \"bias_rho\": \"rho_bias\"\n}</li>\n<li><strong>distribution (Type[AbstractVariable]):</strong>  The class used to create weight/bias distributions.</li>\n<li><strong>requires_grad (bool, optional):</strong>  If True, gradients will be computed on <code>mu</code> and <code>rho</code>.</li>\n<li><strong>get_layers_func (Callable, optional):</strong>  Layer iteration function.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>DistributionT: A dictionary of distributions keyed by layer names.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">attribute_mapping</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">distribution</span><span class=\"p\">:</span> <span class=\"n\">Type</span><span class=\"p\">[</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">get_layers_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">],</span> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">get_torch_layers</span><span class=\"o\">&gt;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.utils.from_bnn", "modulename": "core.distribution.utils", "qualname": "from_bnn", "kind": "function", "doc": "<p>Construct distributions by reading the attributes (e.g., mu_weight, rho_weight, mu_bias, rho_bias)\nfrom layers typically found in BayesianTorch modules.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The Bayesian Torch model containing layer attributes such as mu_weight, rho_weight, etc.</li>\n<li><strong>distribution (Type[AbstractVariable]):</strong>  The subclass of <code>AbstractVariable</code> for each parameter.</li>\n<li><strong>requires_grad (bool, optional):</strong>  If True, allows gradient-based optimization of <code>mu</code> and <code>rho</code>.</li>\n<li><strong>get_layers_func (Callable, optional):</strong>  A function that retrieves BayesianTorch layers. Defaults to <code>get_bayesian_torch_layers</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>DistributionT: A dictionary mapping layer names to weight/bias distributions.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">distribution</span><span class=\"p\">:</span> <span class=\"n\">Type</span><span class=\"p\">[</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">get_layers_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">],</span> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">get_bayesian_torch_layers</span><span class=\"o\">&gt;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.utils.from_copy", "modulename": "core.distribution.utils", "qualname": "from_copy", "kind": "function", "doc": "<p>Create a new distribution by copying <code>mu</code> and <code>rho</code> from an existing distribution.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dist (DistributionT):</strong>  A distribution dictionary to copy from.</li>\n<li><strong>distribution (Type[AbstractVariable]):</strong>  The class to instantiate for each weight/bias.</li>\n<li><strong>requires_grad (bool, optional):</strong>  If True, the new distribution parameters can be updated via gradients.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>DistributionT: A new distribution dictionary with the same layer structure, \n  but new <code>mu</code> and <code>rho</code> parameters cloned from <code>dist</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dist</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">distribution</span><span class=\"p\">:</span> <span class=\"n\">Type</span><span class=\"p\">[</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">requires_grad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.utils.compute_kl", "modulename": "core.distribution.utils", "qualname": "compute_kl", "kind": "function", "doc": "<p>Compute the total KL divergence between two distributions of the same structure.</p>\n\n<p>Each corresponding layer's weight/bias KL is summed to produce a single scalar.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dist1 (DistributionT):</strong>  The first distribution dictionary.</li>\n<li><strong>dist2 (DistributionT):</strong>  The second distribution dictionary.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: A scalar tensor representing the total KL divergence across all layers.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dist1</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">dist2</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.utils.compute_standard_normal_cdf", "modulename": "core.distribution.utils", "qualname": "compute_standard_normal_cdf", "kind": "function", "doc": "<p>Compute the cumulative distribution function (CDF) of a standard normal at point x.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x (float):</strong>  The input value at which to evaluate the standard normal CDF.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>float: The CDF value of the standard normal distribution at x.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.distribution.utils.truncated_normal_fill_tensor", "modulename": "core.distribution.utils", "qualname": "truncated_normal_fill_tensor", "kind": "function", "doc": "<p>Fill a tensor in-place with values drawn from a truncated normal distribution.</p>\n\n<p>The resulting values lie in the interval [a, b], centered around <code>mean</code>\nwith approximate std <code>std</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>tensor (torch.Tensor):</strong>  The tensor to fill.</li>\n<li><strong>mean (float, optional):</strong>  Mean of the desired distribution. Defaults to 0.0.</li>\n<li><strong>std (float, optional):</strong>  Standard deviation of the desired distribution. Defaults to 1.0.</li>\n<li><strong>a (float, optional):</strong>  Lower bound of truncation. Defaults to -2.0.</li>\n<li><strong>b (float, optional):</strong>  Upper bound of truncation. Defaults to 2.0.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor: The same tensor, filled in-place with truncated normal values.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tensor</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">mean</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">std</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mf\">2.0</span>,</span><span class=\"param\">\t<span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">2.0</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.layer", "modulename": "core.layer", "kind": "module", "doc": "<h2 id=\"overview\">Overview</h2>\n\n<p>Contains probabilistic versions of common neural network layers and the \ninfrastructure to attach distributions to them.</p>\n\n<h2 id=\"contents\">Contents</h2>\n\n<ul>\n<li><strong>AbstractProbLayer</strong>: A base class managing probabilistic_mode and sampling</li>\n<li><strong>ProbConv2d, ProbLinear, ProbBatchNorm1d, ProbBatchNorm2d</strong>: \nProbabilistic layers derived from PyTorch</li>\n<li><strong>utils.py</strong> for layer inspection and traversal</li>\n</ul>\n\n<p>These layers replace deterministic parameters with sampled ones, \nsupporting Bayesian inference under the PAC-Bayes framework.</p>\n"}, {"fullname": "core.layer.AbstractProbLayer", "modulename": "core.layer.AbstractProbLayer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.layer.AbstractProbLayer.AbstractProbLayer", "modulename": "core.layer.AbstractProbLayer", "qualname": "AbstractProbLayer", "kind": "class", "doc": "<p>Base class for probabilistic layers in a neural network.</p>\n\n<p>It introduces the concept of <code>probabilistic_mode</code>, which determines whether\na layer samples parameters from a distribution or uses deterministic values.\nEach layer holds references to distributions for weights/biases and their prior.</p>\n", "bases": "torch.nn.modules.module.Module, abc.ABC"}, {"fullname": "core.layer.AbstractProbLayer.AbstractProbLayer.probabilistic_mode", "modulename": "core.layer.AbstractProbLayer", "qualname": "AbstractProbLayer.probabilistic_mode", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool"}, {"fullname": "core.layer.AbstractProbLayer.AbstractProbLayer.probabilistic", "modulename": "core.layer.AbstractProbLayer", "qualname": "AbstractProbLayer.probabilistic", "kind": "function", "doc": "<p>Recursively set the <code>probabilistic_mode</code> flag for this layer and its children.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mode (bool, optional):</strong>  If True, the layer will draw samples from its\ndistributions during forward passes. If False, it will typically\nuse deterministic parameters (i.e., the mean), except in training.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "core.layer.AbstractProbLayer.AbstractProbLayer.sample_from_distribution", "modulename": "core.layer.AbstractProbLayer", "qualname": "AbstractProbLayer.sample_from_distribution", "kind": "function", "doc": "<p>Draw samples for weight and bias from their respective distributions.</p>\n\n<p>Behavior depends on <code>probabilistic_mode</code>:</p>\n\n<ul>\n<li>If <code>probabilistic_mode</code> is True, the method samples from <code>_weight_dist</code>\nand <code>_bias_dist</code>.</li>\n<li>If <code>probabilistic_mode</code> is False and the layer is in eval mode, it uses\nthe mean (mu) of each distribution.</li>\n<li>If <code>probabilistic_mode</code> is False and the layer is in training mode, it\nraises an error, as training requires sampling.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tuple[Tensor, Tensor]: Sampled weight and bias. If the layer has no bias,\n  the second returned value is None.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.layer.ProbBatchNorm1d", "modulename": "core.layer.ProbBatchNorm1d", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.layer.ProbBatchNorm1d.ProbBatchNorm1d", "modulename": "core.layer.ProbBatchNorm1d", "qualname": "ProbBatchNorm1d", "kind": "class", "doc": "<p>A probabilistic 1D batch normalization layer.</p>\n\n<p>This layer extends PyTorch's <code>nn.BatchNorm1d</code> to sample weight and bias\nfrom learned distributions. The forward pass behavior is the same as standard\nbatch norm, except the parameters come from <code>sample_from_distribution</code>.</p>\n", "bases": "torch.nn.modules.batchnorm.BatchNorm1d, core.layer.AbstractProbLayer.AbstractProbLayer"}, {"fullname": "core.layer.ProbBatchNorm1d.ProbBatchNorm1d.forward", "modulename": "core.layer.ProbBatchNorm1d", "qualname": "ProbBatchNorm1d.forward", "kind": "function", "doc": "<p>Forward pass for probabilistic batch normalization.</p>\n\n<h6 id=\"during-training\">During training:</h6>\n\n<blockquote>\n  <ul>\n  <li>Maintains running statistics if <code>track_running_stats</code> is True.</li>\n  <li>Samples weight/bias if <code>probabilistic_mode</code> is True.</li>\n  </ul>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input (Tensor):</strong>  Input tensor of shape (N, C, L) or (N, C).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: Batch-normalized output of the same shape as <code>input</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.layer.ProbBatchNorm2d", "modulename": "core.layer.ProbBatchNorm2d", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.layer.ProbBatchNorm2d.ProbBatchNorm2d", "modulename": "core.layer.ProbBatchNorm2d", "qualname": "ProbBatchNorm2d", "kind": "class", "doc": "<p>A probabilistic 2D batch normalization layer.</p>\n\n<p>Extends PyTorch's <code>nn.BatchNorm2d</code> to sample weight and bias from learned\ndistributions for use in a probabilistic neural network.</p>\n", "bases": "torch.nn.modules.batchnorm.BatchNorm2d, core.layer.AbstractProbLayer.AbstractProbLayer"}, {"fullname": "core.layer.ProbBatchNorm2d.ProbBatchNorm2d.forward", "modulename": "core.layer.ProbBatchNorm2d", "qualname": "ProbBatchNorm2d.forward", "kind": "function", "doc": "<p>Forward pass for probabilistic 2D batch normalization.</p>\n\n<h6 id=\"during-training\">During training:</h6>\n\n<blockquote>\n  <ul>\n  <li>Uses mini-batch statistics to normalize.</li>\n  <li>Samples weight and bias if <code>probabilistic_mode</code> is True.</li>\n  </ul>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input (Tensor):</strong>  Input tensor of shape (N, C, H, W).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: Batch-normalized output of the same shape as <code>input</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.layer.ProbConv2d", "modulename": "core.layer.ProbConv2d", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.layer.ProbConv2d.ProbConv2d", "modulename": "core.layer.ProbConv2d", "qualname": "ProbConv2d", "kind": "class", "doc": "<p>A probabilistic 2D convolution layer.</p>\n\n<p>Inherits from <code>nn.Conv2d</code> and <code>AbstractProbLayer</code>. Weights and bias\nare sampled from associated distributions during forward passes.</p>\n", "bases": "torch.nn.modules.conv.Conv2d, core.layer.AbstractProbLayer.AbstractProbLayer"}, {"fullname": "core.layer.ProbConv2d.ProbConv2d.forward", "modulename": "core.layer.ProbConv2d", "qualname": "ProbConv2d.forward", "kind": "function", "doc": "<p>Perform a 2D convolution using sampled weights and bias.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input (Tensor):</strong>  The input tensor of shape (N, C_in, H_in, W_in).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: The output tensor of shape (N, C_out, H_out, W_out).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.layer.ProbLinear", "modulename": "core.layer.ProbLinear", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.layer.ProbLinear.ProbLinear", "modulename": "core.layer.ProbLinear", "qualname": "ProbLinear", "kind": "class", "doc": "<p>A probabilistic linear (fully connected) layer.</p>\n\n<p>Extends <code>nn.Linear</code> such that weights and bias are sampled from\na distribution during each forward pass if <code>probabilistic_mode</code> is True.</p>\n", "bases": "torch.nn.modules.linear.Linear, core.layer.AbstractProbLayer.AbstractProbLayer"}, {"fullname": "core.layer.ProbLinear.ProbLinear.forward", "modulename": "core.layer.ProbLinear", "qualname": "ProbLinear.forward", "kind": "function", "doc": "<p>Forward pass for a probabilistic linear layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input (Tensor):</strong>  Input tensor of shape (N, in_features).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: Output tensor of shape (N, out_features).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.layer.supported_layers", "modulename": "core.layer.supported_layers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.layer.supported_layers.LAYER_MAPPING", "modulename": "core.layer.supported_layers", "qualname": "LAYER_MAPPING", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;: &lt;class &#x27;core.layer.ProbLinear.ProbLinear&#x27;&gt;, &lt;class &#x27;torch.nn.modules.conv.Conv2d&#x27;&gt;: &lt;class &#x27;core.layer.ProbConv2d.ProbConv2d&#x27;&gt;, &lt;class &#x27;torch.nn.modules.batchnorm.BatchNorm1d&#x27;&gt;: &lt;class &#x27;core.layer.ProbBatchNorm1d.ProbBatchNorm1d&#x27;&gt;, &lt;class &#x27;torch.nn.modules.batchnorm.BatchNorm2d&#x27;&gt;: &lt;class &#x27;core.layer.ProbBatchNorm2d.ProbBatchNorm2d&#x27;&gt;}"}, {"fullname": "core.layer.utils", "modulename": "core.layer.utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.layer.utils.LayerNameT", "modulename": "core.layer.utils", "qualname": "LayerNameT", "kind": "variable", "doc": "<p></p>\n", "default_value": "typing.Tuple[str, ...]"}, {"fullname": "core.layer.utils.get_layers", "modulename": "core.layer.utils", "qualname": "get_layers", "kind": "function", "doc": "<p>Recursively traverse a PyTorch model to find layers matching a given criterion.</p>\n\n<p>This function performs a depth-first search over children of <code>model</code>.\nIf <code>is_layer_func(layer)</code> is True, yield the path of layer names (as a tuple)\nand the layer object.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The PyTorch model or submodule to traverse.</li>\n<li><strong>is_layer_func (Callable):</strong>  A predicate function that returns True if a layer\nmatches the criterion (e.g., belongs to a certain set of layer types).</li>\n<li><strong>names (List[str]):</strong>  Accumulates the hierarchical names as we recurse.</li>\n</ul>\n\n<h6 id=\"yields\">Yields:</h6>\n\n<blockquote>\n  <p>Iterator[Tuple[LayerNameT, nn.Module]]: Tuples of (layer_name_path, layer_object).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">is_layer_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">],</span> <span class=\"nb\">bool</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">names</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.layer.utils.is_torch_layer", "modulename": "core.layer.utils", "qualname": "is_torch_layer", "kind": "function", "doc": "<p>Check if the given layer is a supported PyTorch layer in the framework.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer (nn.Module):</strong>  A PyTorch layer or module.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>bool: True if the layer's type is one of the framework's supported mappings.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">layer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.layer.utils.get_torch_layers", "modulename": "core.layer.utils", "qualname": "get_torch_layers", "kind": "function", "doc": "<p>Yield all supported PyTorch layers in the model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The PyTorch model to traverse.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Iterator[Tuple[LayerNameT, nn.Module]]: Each tuple is (path_of_names, layer).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.layer.utils.is_bayesian_torch_layer", "modulename": "core.layer.utils", "qualname": "is_bayesian_torch_layer", "kind": "function", "doc": "<p>Check if the layer belongs to a BayesianTorch-style layer,\nidentified by having 'mu_weight', 'rho_weight', 'mu_bias', 'rho_bias'\nor the 'kernel' equivalent attributes.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer (nn.Module):</strong>  A PyTorch module.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>bool: True if the layer has BayesianTorch parameter attributes.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">layer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.layer.utils.get_bayesian_torch_layers", "modulename": "core.layer.utils", "qualname": "get_bayesian_torch_layers", "kind": "function", "doc": "<p>Yield all layers in the model recognized as BayesianTorch layers,\ni.e., those containing 'mu_weight', 'rho_weight', etc.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The PyTorch model to traverse.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Iterator[Tuple[LayerNameT, nn.Module]]: (layer_name_path, layer_object) for each Bayesian layer.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.loss", "modulename": "core.loss", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.loss.rescale_loss", "modulename": "core.loss", "qualname": "rescale_loss", "kind": "function", "doc": "<p>Rescale a loss value by dividing it by log(1/pmin).</p>\n\n<p>This is often used in PAC-Bayes settings to keep losses within a certain range \n(e.g., converting losses to the [0, 1] interval) or to improve numerical stability \nwhen probabilities must be bounded below by <code>pmin</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loss (Tensor):</strong>  A scalar or batched loss tensor.</li>\n<li><strong>pmin (float):</strong>  A lower bound for probabilities (e.g., 1e-5).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: The loss tensor divided by ln(1/pmin).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">pmin</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.loss.nll_loss", "modulename": "core.loss", "qualname": "nll_loss", "kind": "function", "doc": "<p>Compute the negative log-likelihood (NLL) loss for classification.</p>\n\n<p>In typical classification settings, <code>outputs</code> is the log-probability of each class \n(e.g., from a log-softmax layer), and <code>targets</code> are the true class indices.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>outputs (Tensor):</strong>  Log-probabilities of shape (batch_size, num_classes).</li>\n<li><strong>targets (Tensor):</strong>  Ground truth class indices of shape (batch_size,).</li>\n<li><strong>pmin (float, optional):</strong>  Not used directly here; kept for uniform interface with other loss functions.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: A scalar tensor representing the average NLL loss over the batch.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">targets</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">pmin</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.loss.scaled_nll_loss", "modulename": "core.loss", "qualname": "scaled_nll_loss", "kind": "function", "doc": "<p>Compute the negative log-likelihood (NLL) loss and then rescale it by log(1/pmin).</p>\n\n<p>This is a combination of <code>nll_loss</code> and <code>rescale_loss</code>, often used to ensure \nthat the final loss remains within a desired numeric range for PAC-Bayes optimization.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>outputs (Tensor):</strong>  Log-probabilities of shape (batch_size, num_classes).</li>\n<li><strong>targets (Tensor):</strong>  Ground truth class indices of shape (batch_size,).</li>\n<li><strong>pmin (float):</strong>  A lower bound for probabilities.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: The rescaled NLL loss as a scalar tensor.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">targets</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">pmin</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.loss.zero_one_loss", "modulename": "core.loss", "qualname": "zero_one_loss", "kind": "function", "doc": "<p>Compute the 0-1 classification error.</p>\n\n<p>This function returns a loss between 0 and 1, where 0 indicates perfect \nclassification on the given batch and 1 indicates total misclassification.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>outputs (Tensor):</strong>  Logits or log-probabilities for each class.</li>\n<li><strong>targets (Tensor):</strong>  Ground truth class indices of shape (batch_size,).</li>\n<li><strong>pmin (float, optional):</strong>  Not used here; kept for consistency with other losses.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: A single-element tensor with the 0-1 error (1 - accuracy).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">targets</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">pmin</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.loss.compute_losses", "modulename": "core.loss", "qualname": "compute_losses", "kind": "function", "doc": "<p>Compute average losses over multiple Monte Carlo samples for a given dataset.</p>\n\n<p>This function is typically used to estimate the expected risk under the \nposterior by sampling the model <code>mc_samples</code> times for each batch in the <code>bound_loader</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  A probabilistic neural network model.</li>\n<li><strong>bound_loader (DataLoader):</strong>  A DataLoader for the dataset on which \nthe losses should be computed (e.g. a bound or test set).</li>\n<li><strong>mc_samples (int):</strong>  Number of Monte Carlo samples to draw from the posterior \nfor each batch.</li>\n<li><strong>loss_func_list (List[Callable]):</strong>  List of loss functions to evaluate \n(e.g., [nll_loss, zero_one_loss]).</li>\n<li><strong>device (torch.device):</strong>  The device (CPU/GPU) on which computations are performed.</li>\n<li><strong>pmin (float, optional):</strong>  A lower bound for probabilities. If provided, \n<code>bounded_call</code> will be used to clamp model outputs.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: A tensor of shape (len(loss_func_list),) containing the average losses \n      across the entire dataset for each loss function. The result is typically \n      used to estimate or bound the generalization error in PAC-Bayes experiments.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">bound_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">mc_samples</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">loss_func_list</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">pmin</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.metric", "modulename": "core.metric", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.metric.evaluate_metrics", "modulename": "core.metric", "qualname": "evaluate_metrics", "kind": "function", "doc": "<p>Evaluate a set of metric functions on a test set with multiple Monte Carlo samples.</p>\n\n<p>This function uses <code>compute_losses</code> under the hood to compute each metric\n(e.g., NLL, 0-1 error) over <code>num_samples_metric</code> samples from the posterior.\nOptionally logs the results to Weights &amp; Biases (wandb).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  A probabilistic neural network model.</li>\n<li><strong>metrics (Dict[str, Callable]):</strong>  A dictionary mapping metric names \nto metric functions (e.g., {\"zero_one\": zero_one_loss}).</li>\n<li><strong>test_loader (DataLoader):</strong>  DataLoader for the test/validation dataset.</li>\n<li><strong>num_samples_metric (int):</strong>  Number of Monte Carlo samples to draw \nwhen evaluating each metric on the test set.</li>\n<li><strong>device (torch.device):</strong>  The device (CPU/GPU) to run computations on.</li>\n<li><strong>pmin (float, optional):</strong>  A lower bound for probabilities. If specified, \n<code>bounded_call</code> is applied to model outputs.</li>\n<li><strong>wandb_params (Dict, optional):</strong>  Configuration for logging to wandb. \nExpects keys:\n<ul>\n<li>\"log_wandb\": bool, whether to log or not</li>\n<li>\"name_wandb\": str, prefix for logging metrics</li>\n</ul></li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Dict[str, Tensor]: A dictionary mapping each metric name to its average value \n  across the entire test dataset and all Monte Carlo samples.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">metrics</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Callable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">test_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples_metric</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">pmin</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-05</span>,</span><span class=\"param\">\t<span class=\"n\">wandb_params</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.model", "modulename": "core.model", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.model.bounded_call", "modulename": "core.model", "qualname": "bounded_call", "kind": "function", "doc": "<p>Forward data through the model and clamp the output to a minimum log-probability.</p>\n\n<p>This is typically used to avoid numerical instability in PAC-Bayes experiments \nwhen dealing with small probabilities. The output is clamped at log(pmin) to \nensure log-probabilities do not fall below this threshold.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The (probabilistic) neural network model.</li>\n<li><strong>data (Tensor):</strong>  Input data of shape (batch_size, ...).</li>\n<li><strong>pmin (float):</strong>  A lower bound for probabilities. Outputs are clamped at log(pmin).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: Model outputs with each element &gt;= log(pmin).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">pmin</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.model.dnn_to_probnn", "modulename": "core.model", "qualname": "dnn_to_probnn", "kind": "function", "doc": "<p>Convert a deterministic PyTorch model into a probabilistic neural network (ProbNN)\nby attaching weight/bias distributions to its layers.</p>\n\n<p>This function iterates through each layer in the model (using <code>get_layers_func</code>), \nand if the layer type is supported, it:</p>\n\n<ul>\n<li>Registers prior and posterior distributions for weights and biases.</li>\n<li>Marks the layer as probabilistic (so that it samples weights/biases in forward calls).</li>\n<li>Replaces the layer class with its probabilistic counterpart from <code>LAYER_MAPPING</code>.</li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  A deterministic PyTorch model (e.g., a CNN).</li>\n<li><strong>weight_dist (DistributionT):</strong>  A dictionary containing posterior distributions \nfor weights and biases, keyed by layer name.</li>\n<li><strong>prior_weight_dist (DistributionT):</strong>  A dictionary containing prior distributions \nfor weights and biases, keyed by layer name.</li>\n<li><strong>get_layers_func (Callable):</strong>  A function that returns an iterator of (layer_name, layer_module) \npairs. Defaults to <code>get_torch_layers</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None: The function modifies <code>model</code> in place, converting certain layers \n  to their probabilistic equivalents.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">weight_dist</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prior_weight_dist</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">get_layers_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">],</span> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">get_torch_layers</span><span class=\"o\">&gt;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "core.model.update_dist", "modulename": "core.model", "qualname": "update_dist", "kind": "function", "doc": "<p>Update the weight/bias distributions of an already converted probabilistic model.</p>\n\n<p>This is useful when you want to load a different set of posterior or prior \ndistributions into the same network structure, without re-running the entire \n<code>dnn_to_probnn</code> procedure.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The probabilistic neural network model (already converted).</li>\n<li><strong>weight_dist (DistributionT, optional):</strong>  New posterior distributions keyed by layer name.\nIf provided, each layer's '_weight_dist' and '_bias_dist' are updated.</li>\n<li><strong>prior_weight_dist (DistributionT, optional):</strong>  New prior distributions keyed by layer name.\nIf provided, each layer's '_prior_weight_dist' and '_prior_bias_dist' are updated.</li>\n<li><strong>get_layers_func (Callable):</strong>  Function that returns an iterator of (layer_name, layer_module).\nDefaults to <code>get_torch_layers</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None: The distributions in the model are updated in place.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">weight_dist</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prior_weight_dist</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">get_layers_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">],</span> <span class=\"n\">Iterator</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">get_torch_layers</span><span class=\"o\">&gt;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "core.objective", "modulename": "core.objective", "kind": "module", "doc": "<h2 id=\"overview\">Overview</h2>\n\n<p>Hosts a variety of PAC-Bayes training objectives that combine empirical \nloss and KL divergence in different ways.</p>\n\n<h2 id=\"contents\">Contents</h2>\n\n<ul>\n<li><strong>AbstractObjective</strong>: A base interface </li>\n<li><strong>BBBObjective, FClassicObjective, FQuadObjective, McAllisterObjective, TolstikhinObjective</strong></li>\n</ul>\n\n<p>By choosing an appropriate objective, you can guide training to minimize \na PAC-Bayes bound on the model\u2019s risk.</p>\n"}, {"fullname": "core.objective.AbstractObjective", "modulename": "core.objective.AbstractObjective", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.objective.AbstractObjective.AbstractObjective", "modulename": "core.objective.AbstractObjective", "qualname": "AbstractObjective", "kind": "class", "doc": "<p>Base class for PAC-Bayes training objectives.</p>\n\n<h6 id=\"an-objective-typically-combines\">An objective typically combines:</h6>\n\n<blockquote>\n  <ul>\n  <li>Empirical loss (e.g., negative log-likelihood)</li>\n  <li>KL divergence between posterior and prior</li>\n  <li>Additional terms for confidence or other bounding factors</li>\n  </ul>\n</blockquote>\n", "bases": "abc.ABC"}, {"fullname": "core.objective.AbstractObjective.AbstractObjective.calculate", "modulename": "core.objective.AbstractObjective", "qualname": "AbstractObjective.calculate", "kind": "function", "doc": "<p>Compute the combined objective scalar to be backpropagated.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loss (Tensor):</strong>  Empirical loss, e.g. cross-entropy on a batch.</li>\n<li><strong>kl (Tensor):</strong>  KL divergence between the current posterior and prior.</li>\n<li><strong>num_samples (float):</strong>  Number of samples used or total dataset size,\nused for scaling KL or other terms.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: A scalar tensor that includes the loss, KL penalty, and any other terms.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">kl</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.objective.BBBObjective", "modulename": "core.objective.BBBObjective", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.objective.BBBObjective.BBBObjective", "modulename": "core.objective.BBBObjective", "qualname": "BBBObjective", "kind": "class", "doc": "<p>The Bayes By Backprop (BBB) objective from Blundell et al. (2015).</p>\n\n<p>This objective typically adds a KL penalty weighted by a user-defined factor.</p>\n", "bases": "core.objective.AbstractObjective.AbstractObjective"}, {"fullname": "core.objective.BBBObjective.BBBObjective.__init__", "modulename": "core.objective.BBBObjective", "qualname": "BBBObjective.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>kl_penalty (float):</strong>  The coefficient for scaling KL divergence in the objective.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">kl_penalty</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "core.objective.BBBObjective.BBBObjective.calculate", "modulename": "core.objective.BBBObjective", "qualname": "BBBObjective.calculate", "kind": "function", "doc": "<p>Combine the loss with scaled KL divergence.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loss (Tensor):</strong>  Empirical loss (e.g., NLL).</li>\n<li><strong>kl (Tensor):</strong>  KL divergence between posterior and prior.</li>\n<li><strong>num_samples (float):</strong>  The number of training samples or an equivalent factor.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: A scalar objective = loss + (kl_penalty * KL / num_samples).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">kl</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.objective.FClassicObjective", "modulename": "core.objective.FClassicObjective", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.objective.FClassicObjective.FClassicObjective", "modulename": "core.objective.FClassicObjective", "qualname": "FClassicObjective", "kind": "class", "doc": "<p>The \"f-classic\" objective from Perez-Ortiz et al. (2021), which\ncombines empirical loss with a square-root bounding term involving KL and delta.</p>\n\n<p>Typically used to ensure a PAC-Bayes bound is minimized during training.</p>\n", "bases": "core.objective.AbstractObjective.AbstractObjective"}, {"fullname": "core.objective.FClassicObjective.FClassicObjective.__init__", "modulename": "core.objective.FClassicObjective", "qualname": "FClassicObjective.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>kl_penalty (float):</strong>  Coefficient for scaling KL divergence.</li>\n<li><strong>delta (float):</strong>  Confidence parameter for the PAC-Bayes bound.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">kl_penalty</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "core.objective.FClassicObjective.FClassicObjective.calculate", "modulename": "core.objective.FClassicObjective", "qualname": "FClassicObjective.calculate", "kind": "function", "doc": "<p>Compute the f-classic objective.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loss (Tensor):</strong>  Empirical risk (e.g., average loss on a mini-batch).</li>\n<li><strong>kl (Tensor):</strong>  KL divergence between posterior and prior.</li>\n<li><strong>num_samples (float):</strong>  Number of samples or an equivalent scaling factor.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: A scalar objective = loss + sqrt( (KL * kl_penalty + ln(2 sqrt(n)/delta)) / (2n) ).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">kl</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.objective.FQuadObjective", "modulename": "core.objective.FQuadObjective", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.objective.FQuadObjective.FQuadObjective", "modulename": "core.objective.FQuadObjective", "qualname": "FQuadObjective", "kind": "class", "doc": "<p>A \"f-quad\" objective from Perez-Ortiz et al. (2021), which involves\na quadratic expression derived from the PAC-Bayes bound.</p>\n", "bases": "core.objective.AbstractObjective.AbstractObjective"}, {"fullname": "core.objective.FQuadObjective.FQuadObjective.__init__", "modulename": "core.objective.FQuadObjective", "qualname": "FQuadObjective.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>kl_penalty (float):</strong>  Coefficient to scale the KL term.</li>\n<li><strong>delta (float):</strong>  Confidence parameter for the PAC-Bayes bound.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">kl_penalty</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "core.objective.FQuadObjective.FQuadObjective.calculate", "modulename": "core.objective.FQuadObjective", "qualname": "FQuadObjective.calculate", "kind": "function", "doc": "<p>Compute the f-quad objective.</p>\n\n<h6 id=\"this-objective-calculates\">This objective calculates:</h6>\n\n<blockquote>\n  <p>( sqrt(loss + ratio) + sqrt(ratio) )^2</p>\n</blockquote>\n\n<p>where ratio = (KL + ln(2 sqrt(n)/delta)) / (2n).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loss (Tensor):</strong>  Empirical loss.</li>\n<li><strong>kl (Tensor):</strong>  KL divergence.</li>\n<li><strong>num_samples (float):</strong>  Dataset size or similar factor.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: The scalar objective value.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">kl</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.objective.McAllisterObjective", "modulename": "core.objective.McAllisterObjective", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.objective.McAllisterObjective.McAllisterObjective", "modulename": "core.objective.McAllisterObjective", "qualname": "McAllisterObjective", "kind": "class", "doc": "<p>McAllister bound objective (based on McAllister, 1999 and related works),\ncombining empirical loss with a square-root term involving KL and delta,\nplus additional constants (e.g., 5/2 ln(n)) in the bounding expression.</p>\n", "bases": "core.objective.AbstractObjective.AbstractObjective"}, {"fullname": "core.objective.McAllisterObjective.McAllisterObjective.__init__", "modulename": "core.objective.McAllisterObjective", "qualname": "McAllisterObjective.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>kl_penalty (float):</strong>  Multiplier for the KL divergence term.</li>\n<li><strong>delta (float):</strong>  Confidence parameter in the PAC-Bayes bound.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">kl_penalty</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "core.objective.McAllisterObjective.McAllisterObjective.calculate", "modulename": "core.objective.McAllisterObjective", "qualname": "McAllisterObjective.calculate", "kind": "function", "doc": "<p>Compute the McAllister objective.</p>\n\n<p>Derived from a PAC-Bayes bound that includes terms like ln(num_samples)\nand -ln(delta).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loss (Tensor):</strong>  Empirical loss or risk.</li>\n<li><strong>kl (Tensor):</strong>  KL divergence.</li>\n<li><strong>num_samples (float):</strong>  Number of samples/training size.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: A scalar objective = loss + sqrt( [kl_penalty*KL + 5/2 ln(n) - ln(delta) + 8 ] / [2n - 1] ).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">kl</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.objective.TolstikhinObjective", "modulename": "core.objective.TolstikhinObjective", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.objective.TolstikhinObjective.TolstikhinObjective", "modulename": "core.objective.TolstikhinObjective", "qualname": "TolstikhinObjective", "kind": "class", "doc": "<p>Objective related to Tolstikhin et al. (2013), featuring a combination of\nthe empirical loss, a square-root term involving KL, and an additional additive term.</p>\n", "bases": "core.objective.AbstractObjective.AbstractObjective"}, {"fullname": "core.objective.TolstikhinObjective.TolstikhinObjective.__init__", "modulename": "core.objective.TolstikhinObjective", "qualname": "TolstikhinObjective.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>kl_penalty (float):</strong>  The coefficient multiplying the KL term.</li>\n<li><strong>delta (float):</strong>  Confidence parameter in the PAC-Bayes or related bound.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">kl_penalty</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "core.objective.TolstikhinObjective.TolstikhinObjective.calculate", "modulename": "core.objective.TolstikhinObjective", "qualname": "TolstikhinObjective.calculate", "kind": "function", "doc": "<p>Compute the Tolstikhin objective.</p>\n\n<h6 id=\"the-final-expression-includes\">The final expression includes:</h6>\n\n<blockquote>\n  <p>loss + sqrt(2 * loss * ratio) + 2 * ratio</p>\n</blockquote>\n\n<p>where ratio = (kl_penalty*KL + ln(2n) - ln(delta)) / (2n).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loss (Tensor):</strong>  Empirical loss.</li>\n<li><strong>kl (Tensor):</strong>  KL divergence.</li>\n<li><strong>num_samples (float):</strong>  Number of data samples for normalization.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tensor: The scalar objective value.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">kl</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.risk", "modulename": "core.risk", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.risk.certify_risk", "modulename": "core.risk", "qualname": "certify_risk", "kind": "function", "doc": "<p>Certify (evaluate) the generalization risk of a probabilistic neural network\nusing one or more PAC-Bayes bounds on a given dataset.</p>\n\n<h6 id=\"steps\">Steps:</h6>\n\n<blockquote>\n  <p>1) Compute average losses (e.g., NLL, 0-1 error) via multiple Monte Carlo samples\n     from the posterior (<code>compute_losses</code>).\n  2) Calculate the KL divergence between the posterior and prior distributions.\n  3) For each bound in <code>bounds</code>, calculate a PAC-Bayes risk bound for each loss in <code>losses</code>.\n  4) Optionally log intermediate results (loss, risk) to Weights &amp; Biases (wandb).</p>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The probabilistic neural network used for risk evaluation.</li>\n<li><strong>bounds (Dict[str, AbstractBound]):</strong>  A mapping from bound names to bound objects \nthat implement a PAC-Bayes bound (<code>AbstractBound</code>).</li>\n<li><strong>losses (Dict[str, Callable]):</strong>  A mapping from loss names to loss functions \n(e.g., {\"nll\": nll_loss, \"01\": zero_one_loss}).</li>\n<li><strong>posterior (DistributionT):</strong>  Posterior distribution of the model parameters.</li>\n<li><strong>prior (DistributionT):</strong>  Prior distribution of the model parameters.</li>\n<li><strong>bound_loader (DataLoader):</strong>  DataLoader for the dataset on which bounds and losses are computed.</li>\n<li><strong>num_samples_loss (int):</strong>  Number of Monte Carlo samples to draw from the posterior\nfor estimating the average losses.</li>\n<li><strong>device (torch.device):</strong>  The device (CPU or GPU) to perform computations on.</li>\n<li><strong>pmin (float, optional):</strong>  A minimum probability bound for clamping model outputs in log space.\nDefaults to 1e-5.</li>\n<li><strong>wandb_params (Dict, optional):</strong>  Configuration for Weights &amp; Biases logging. Expects keys:\n<ul>\n<li>\"log_wandb\": bool, whether to log</li>\n<li>\"name_wandb\": str, prefix for metric names</li>\n</ul></li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Dict[str, Dict[str, Dict[str, Tensor]]]: A nested dictionary of the form:\n      {\n        bound_name: {\n          loss_name: {\n            'risk': risk_value,\n            'loss': avg_loss_value\n          }\n        }\n      }\n  where <code>risk_value</code> is the computed bound on the risk, and <code>avg_loss_value</code> is the\n  empirical loss estimate for that loss and bound.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">bounds</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">bound</span><span class=\"o\">.</span><span class=\"n\">AbstractBound</span><span class=\"o\">.</span><span class=\"n\">AbstractBound</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">losses</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Callable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">posterior</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prior</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">bound_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples_loss</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">pmin</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-05</span>,</span><span class=\"param\">\t<span class=\"n\">wandb_params</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.split_strategy", "modulename": "core.split_strategy", "kind": "module", "doc": "<h2 id=\"overview\">Overview</h2>\n\n<p>Defines how to split datasets into parts for prior training, posterior \ntraining, bound evaluation, validation, and testing in PAC-Bayes pipelines.</p>\n\n<h2 id=\"contents\">Contents</h2>\n\n<ul>\n<li><strong>AbstractSplitStrategy</strong>: The base interface</li>\n<li><strong>PBPSplitStrategy, FaultySplitStrategy</strong>: Concrete implementations \nto partition data for different training/evaluation scenarios</li>\n</ul>\n\n<p>Use these strategies to ensure data for prior and posterior does not overlap \nand to reserve a portion for bound computation.</p>\n"}, {"fullname": "core.split_strategy.AbstractSplitStrategy", "modulename": "core.split_strategy.AbstractSplitStrategy", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.split_strategy.AbstractSplitStrategy.AbstractSplitStrategy", "modulename": "core.split_strategy.AbstractSplitStrategy", "qualname": "AbstractSplitStrategy", "kind": "class", "doc": "<p>An abstract interface for splitting a dataset loader into separate subsets\nfor training, prior training, validation, testing, or bound evaluation\nin a PAC-Bayes pipeline.</p>\n\n<p>Different implementations can define how the data is partitioned,\nensuring that prior data, posterior data, and bound data do not overlap.</p>\n", "bases": "abc.ABC"}, {"fullname": "core.split_strategy.AbstractSplitStrategy.AbstractSplitStrategy.split", "modulename": "core.split_strategy.AbstractSplitStrategy", "qualname": "AbstractSplitStrategy.split", "kind": "function", "doc": "<p>Partition the data from <code>dataset_loader</code> according to the configuration in <code>split_config</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dataset_loader (AbstractLoader):</strong>  A loader or dataset manager providing the raw dataset.</li>\n<li><strong>split_config (Dict):</strong>  A dictionary specifying how to split the data \n(e.g., batch_size, train/val/test percentages, random seeds, etc.).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None: The method typically sets up instance attributes such as\n  <code>posterior_loader</code>, <code>prior_loader</code>, etc. for later access.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_loader</span><span class=\"p\">:</span> <span class=\"n\">scripts</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">loader</span><span class=\"o\">.</span><span class=\"n\">AbstractLoader</span><span class=\"o\">.</span><span class=\"n\">AbstractLoader</span>,</span><span class=\"param\">\t<span class=\"n\">split_config</span><span class=\"p\">:</span> <span class=\"n\">Dict</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.split_strategy.FaultySplitStrategy", "modulename": "core.split_strategy.FaultySplitStrategy", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.split_strategy.FaultySplitStrategy.FaultySplitStrategy", "modulename": "core.split_strategy.FaultySplitStrategy", "qualname": "FaultySplitStrategy", "kind": "class", "doc": "<p>A specialized (and potentially buggy) subclass of PBPSplitStrategy that demonstrates\nalternative splitting logic or partial overlaps between dataset subsets.</p>\n\n<h6 id=\"fields\">Fields:</h6>\n\n<blockquote>\n  <p>posterior_loader (DataLoader): DataLoader for posterior training.\n  prior_loader (DataLoader): DataLoader for prior training.\n  val_loader (DataLoader): DataLoader for validation set.\n  test_loader (DataLoader): DataLoader for test set.\n  test_1batch (DataLoader): DataLoader for test set (one big batch).\n  bound_loader (DataLoader): DataLoader for bound evaluation set.\n  bound_loader_1batch (DataLoader): DataLoader for bound evaluation set (one big batch).</p>\n</blockquote>\n", "bases": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy"}, {"fullname": "core.split_strategy.FaultySplitStrategy.FaultySplitStrategy.__init__", "modulename": "core.split_strategy.FaultySplitStrategy", "qualname": "FaultySplitStrategy.__init__", "kind": "function", "doc": "<p>Initialize the FaultySplitStrategy with user-defined percentages\nand flags for how to partition the dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>prior_type (str):</strong>  A string indicating how the prior is handled (e.g. \"not_learnt\", \"learnt\").</li>\n<li><strong>train_percent (float):</strong>  Fraction of dataset to use for training.</li>\n<li><strong>val_percent (float):</strong>  Fraction of dataset to use for validation.</li>\n<li><strong>prior_percent (float):</strong>  Fraction of dataset to use for prior training.</li>\n<li><strong>self_certified (bool):</strong>  If True, indicates self-certified approach to data splitting.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">prior_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">train_percent</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">val_percent</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">prior_percent</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">self_certified</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span>)</span>"}, {"fullname": "core.split_strategy.FaultySplitStrategy.FaultySplitStrategy.posterior_loader", "modulename": "core.split_strategy.FaultySplitStrategy", "qualname": "FaultySplitStrategy.posterior_loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.FaultySplitStrategy.FaultySplitStrategy.prior_loader", "modulename": "core.split_strategy.FaultySplitStrategy", "qualname": "FaultySplitStrategy.prior_loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.FaultySplitStrategy.FaultySplitStrategy.val_loader", "modulename": "core.split_strategy.FaultySplitStrategy", "qualname": "FaultySplitStrategy.val_loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.FaultySplitStrategy.FaultySplitStrategy.test_loader", "modulename": "core.split_strategy.FaultySplitStrategy", "qualname": "FaultySplitStrategy.test_loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.FaultySplitStrategy.FaultySplitStrategy.test_1batch", "modulename": "core.split_strategy.FaultySplitStrategy", "qualname": "FaultySplitStrategy.test_1batch", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.FaultySplitStrategy.FaultySplitStrategy.bound_loader", "modulename": "core.split_strategy.FaultySplitStrategy", "qualname": "FaultySplitStrategy.bound_loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.FaultySplitStrategy.FaultySplitStrategy.bound_loader_1batch", "modulename": "core.split_strategy.FaultySplitStrategy", "qualname": "FaultySplitStrategy.bound_loader_1batch", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.PBPSplitStrategy", "modulename": "core.split_strategy.PBPSplitStrategy", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy", "modulename": "core.split_strategy.PBPSplitStrategy", "qualname": "PBPSplitStrategy", "kind": "class", "doc": "<p>A split strategy implementing a Prior-Posterior-Bound (PBP) partition of the dataset.</p>\n\n<h6 id=\"this-strategy-supports-data-splits-for\">This strategy supports data splits for:</h6>\n\n<blockquote>\n  <ul>\n  <li>Posterior training</li>\n  <li>Prior training</li>\n  <li>Validation</li>\n  <li>Test</li>\n  <li>Bound evaluation (data for calculating PAC-Bayes bounds)</li>\n  </ul>\n</blockquote>\n\n<p>By changing the internal splitting methods, one can adapt different scenarios such as:</p>\n\n<ul>\n<li>'not_learnt': The prior is not trained.</li>\n<li>'learnt': The prior is trained on some portion of the data.</li>\n<li>'learnt_with_test': Similar to 'learnt', but includes an explicit test subset.</li>\n</ul>\n", "bases": "core.split_strategy.AbstractSplitStrategy.AbstractSplitStrategy"}, {"fullname": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy.__init__", "modulename": "core.split_strategy.PBPSplitStrategy", "qualname": "PBPSplitStrategy.__init__", "kind": "function", "doc": "<p>Initialize the PBPSplitStrategy with user-defined parameters for how to partition the data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>prior_type (str):</strong>  Indicates whether the prior is \"not_learnt\", \"learnt\", or \"learnt_with_test\".</li>\n<li><strong>train_percent (float):</strong>  Proportion of data used for training.</li>\n<li><strong>val_percent (float):</strong>  Proportion of data used for validation.</li>\n<li><strong>prior_percent (float):</strong>  Proportion of data used specifically for training the prior.</li>\n<li><strong>self_certified (bool):</strong>  If True, indicates self-certified splitting approach.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">prior_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">train_percent</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">val_percent</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">prior_percent</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">self_certified</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span>)</span>"}, {"fullname": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy.posterior_loader", "modulename": "core.split_strategy.PBPSplitStrategy", "qualname": "PBPSplitStrategy.posterior_loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy.prior_loader", "modulename": "core.split_strategy.PBPSplitStrategy", "qualname": "PBPSplitStrategy.prior_loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy.val_loader", "modulename": "core.split_strategy.PBPSplitStrategy", "qualname": "PBPSplitStrategy.val_loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy.test_loader", "modulename": "core.split_strategy.PBPSplitStrategy", "qualname": "PBPSplitStrategy.test_loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy.test_1batch", "modulename": "core.split_strategy.PBPSplitStrategy", "qualname": "PBPSplitStrategy.test_1batch", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy.bound_loader", "modulename": "core.split_strategy.PBPSplitStrategy", "qualname": "PBPSplitStrategy.bound_loader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy.bound_loader_1batch", "modulename": "core.split_strategy.PBPSplitStrategy", "qualname": "PBPSplitStrategy.bound_loader_1batch", "kind": "variable", "doc": "<p></p>\n", "annotation": ": torch.utils.data.dataloader.DataLoader", "default_value": "None"}, {"fullname": "core.split_strategy.PBPSplitStrategy.PBPSplitStrategy.split", "modulename": "core.split_strategy.PBPSplitStrategy", "qualname": "PBPSplitStrategy.split", "kind": "function", "doc": "<p>Public method to perform the split operation on a dataset loader,\nsetting up DataLoaders for prior, posterior, validation, testing, and bound evaluation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dataset_loader (Union[MNISTLoader, CIFAR10Loader]):</strong>  A dataset loader instance\nproviding <code>load(dataset_loader_seed)</code> to retrieve train/test datasets.</li>\n<li><strong>split_config (Dict):</strong>  Configuration parameters for splitting (e.g., batch_size, seed).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_loader</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">scripts</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">loader</span><span class=\"o\">.</span><span class=\"n\">MNISTLoader</span><span class=\"o\">.</span><span class=\"n\">MNISTLoader</span><span class=\"p\">,</span> <span class=\"n\">scripts</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">loader</span><span class=\"o\">.</span><span class=\"n\">CIFAR10Loader</span><span class=\"o\">.</span><span class=\"n\">CIFAR10Loader</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">split_config</span><span class=\"p\">:</span> <span class=\"n\">Dict</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.training", "modulename": "core.training", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.training.train", "modulename": "core.training", "qualname": "train", "kind": "function", "doc": "<p>Train a probabilistic neural network by optimizing a PAC-Bayes-inspired objective.</p>\n\n<h6 id=\"at-each-iteration\">At each iteration:</h6>\n\n<blockquote>\n  <p>1) Optionally clamp model outputs using <code>bounded_call</code> if <code>pmin</code> is provided in <code>parameters</code>.\n  2) Compute KL divergence between posterior and prior.\n  3) Compute the empirical loss (NLL by default).\n  4) Combine loss and KL via the given <code>objective</code>.\n  5) Backpropagate and update model parameters.</p>\n</blockquote>\n\n<p>Logs intermediate results (objective, loss, KL) to Python's logger and optionally to wandb.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model (nn.Module):</strong>  The probabilistic neural network to train.</li>\n<li><strong>posterior (DistributionT):</strong>  The current (learnable) posterior distribution.</li>\n<li><strong>prior (DistributionT):</strong>  The (fixed or partially learnable) prior distribution.</li>\n<li><strong>objective (AbstractObjective):</strong>  An object that merges empirical loss and KL \ninto a single differentiable objective.</li>\n<li><strong>train_loader (DataLoader):</strong>  Dataloader for the training dataset.</li>\n<li><strong>val_loader (DataLoader):</strong>  Dataloader for the validation dataset (currently unused here).</li>\n<li><strong>parameters (Dict[str, Any]):</strong>  A dictionary of training hyperparameters, which can include:\n<ul>\n<li>'lr': Learning rate.</li>\n<li>'momentum': Momentum term for SGD.</li>\n<li>'epochs': Number of epochs.</li>\n<li>'num_samples': Usually the size of the training set (or mini-batch size times steps).</li>\n<li>'seed': Random seed (optional).</li>\n<li>'pmin': Minimum probability for bounding (optional).</li>\n</ul></li>\n<li><strong>device (torch.device):</strong>  The device (CPU or GPU) for training.</li>\n<li><strong>wandb_params (Dict, optional):</strong>  Configuration for logging to Weights &amp; Biases. Expects keys:\n<ul>\n<li>\"log_wandb\": bool, whether to log or not</li>\n<li>\"name_wandb\": str, run name / prefix for logging</li>\n</ul></li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None: The model (and its posterior) are updated in-place over the specified epochs.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">posterior</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prior</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"o\">.</span><span class=\"n\">AbstractVariable</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">objective</span><span class=\"p\">:</span> <span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">objective</span><span class=\"o\">.</span><span class=\"n\">AbstractObjective</span><span class=\"o\">.</span><span class=\"n\">AbstractObjective</span>,</span><span class=\"param\">\t<span class=\"n\">train_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">val_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">parameters</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">wandb_params</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "core.utils", "modulename": "core.utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.utils.kl", "modulename": "core.utils.kl", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "core.utils.kl.KLDivergenceInterface", "modulename": "core.utils.kl", "qualname": "KLDivergenceInterface", "kind": "class", "doc": "<p>An abstract base class for computing Kullback-Leibler Divergence (KL Divergence).</p>\n", "bases": "abc.ABC"}, {"fullname": "core.utils.kl.KLDivergenceInterface.compute_kl", "modulename": "core.utils.kl", "qualname": "KLDivergenceInterface.compute_kl", "kind": "function", "doc": "<p>Computes the Kullback-Leibler Divergence (KL Divergence) between two probability distributions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>*args:</strong>  Variable length argument list.</li>\n<li><strong>**kwargs:</strong>  Arbitrary keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>torch.Tensor: The computed KL Divergence.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "core.utils.kl.inv_kl", "modulename": "core.utils.kl", "qualname": "inv_kl", "kind": "function", "doc": "<p>Inversion of the binary KL divergence from (Not) Bounding the True Error by John Langford and Rich Caruana.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>qs (float):</strong>  Empirical risk.</li>\n<li><strong>ks (float):</strong>  Second term for the binary KL divergence inversion.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>float: The computed inversion of the binary KL divergence.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">qs</span>, </span><span class=\"param\"><span class=\"n\">ks</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();